{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dirac equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy \n",
    "from scipy import pi\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshaping(x):\n",
    "    \"\"\"\n",
    "    Reshape the data created with numpy. \n",
    "    \"\"\"\n",
    "    return x.reshape(x.shape[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMALL_SIZE = 20\n",
    "MEDIUM_SIZE = 22\n",
    "BIGGER_SIZE = 24\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N0 = 1000\n",
    "Nb = 1000 # Number of points in the boundaries and initial value.  \n",
    "Nf = 20000 # Number of collocated points.\n",
    "\n",
    "SMALL = 1e-06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space_IV = np.random.uniform(0 + SMALL, 1 - SMALL, N0)\n",
    "space_IV = reshaping(space_IV)\n",
    "x_IV = space_IV\n",
    "t_IV = 0*space_IV\n",
    "u_IV_1 = x_IV**2 - x_IV + 1/6\n",
    "v_IV_1 = 0.0*x_IV\n",
    "u_IV_2 = np.piecewise(x_IV, [x_IV < 0.5, x_IV > 0.5], [lambda x_IV: x_IV - 0.25, lambda x_IV: -x_IV + 0.75])\n",
    "v_IV_2 = 0.0*x_IV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boundary Condition\n",
    "time_BC = np.random.uniform(0 + SMALL, pi/2  - SMALL ,Nb)\n",
    "time_BC = reshaping(time_BC)\n",
    "# Boundary Condition x = 0\n",
    "t_lb = time_BC\n",
    "x_lb = 0*time_BC  \n",
    "u_lb_1 = 0*x_lb\n",
    "# Boundary Condition x = 1\n",
    "t_ub = time_BC\n",
    "x_ub = 0*time_BC + 1\n",
    "u_ub_1 = 0*x_ub\n",
    "# Boundary Condition\n",
    "u_BC_1 = np.concatenate((u_lb_1, u_ub_1), axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boundary Condition x = 0\n",
    "v_lb_1 = 0*x_lb\n",
    "# Boundary Condition x = 1\n",
    "v_ub_1 = 0*x_ub\n",
    "# Boundary Condition\n",
    "v_BC_1 = np.concatenate((v_lb_1, v_ub_1), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boundary Condition x = 0\n",
    "u_lb_2 = 0*x_lb\n",
    "# Boundary Condition x = 1\n",
    "u_ub_2 = 0*x_ub\n",
    "# Boundary Condition\n",
    "u_BC_2 = np.concatenate((u_lb_2, u_ub_2), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boundary Condition x = 0\n",
    "v_lb_2 = 0*x_lb\n",
    "# Boundary Condition x = 1\n",
    "v_ub_2 = 0*x_ub\n",
    "# Boundary Condition\n",
    "v_BC_2 = np.concatenate((v_lb_2, v_ub_2), axis=0)\n",
    "t_BC = np.concatenate((t_lb, t_ub), axis=0)\n",
    "x_BC = np.concatenate((x_lb, x_ub), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_collocated = np.random.uniform(0 + SMALL, 1 - SMALL, Nf)\n",
    "x_collocated = reshaping(x_collocated)\n",
    "t_collocated = np.random.uniform(0 + SMALL, pi/2 - SMALL, Nf)\n",
    "t_collocated = reshaping(t_collocated)\n",
    "xx, tt = np.concatenate((x_collocated, x_IV, x_BC), axis=0), np.concatenate((t_collocated, t_IV, t_BC), axis=0)\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((xx, tt))\n",
    "batch_size = xx.shape[0]\n",
    "train_dataset = train_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_targets = 4\n",
    "n_hidden_layers, n_hidden_neurons = 9,20\n",
    "input_x = tf.keras.Input(shape=(1,))\n",
    "input_t = tf.keras.Input(shape=(1,))\n",
    "inputs = tf.keras.layers.concatenate([input_x, input_t])\n",
    "initializer = tf.keras.initializers.GlorotNormal()\n",
    "x = tf.keras.layers.Dense(n_hidden_neurons, activation='tanh', kernel_initializer=initializer, dtype=tf.float64)(inputs)\n",
    "for hidden in range(n_hidden_layers-1):\n",
    "    initializer = tf.keras.initializers.GlorotNormal()\n",
    "    x = tf.keras.layers.Dense(n_hidden_neurons, activation='tanh', kernel_initializer=initializer, dtype=tf.float64)(x)\n",
    "initializer = tf.keras.initializers.GlorotNormal()\n",
    "outputs = tf.keras.layers.Dense(n_targets, activation='tanh', kernel_initializer=initializer, dtype=tf.float64)(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=[input_x, input_t], outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an optimizer to train the model.\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=1e-02,\n",
    "    decay_steps=100,\n",
    "    decay_rate=0.96)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = tf.keras.losses.MeanSquaredError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(x, t):\n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        # Boundary conditions evaluation\n",
    "        BC_pred = model([x_BC, t_BC], training = True)\n",
    "        u_BC_1_pred = BC_pred[:, 0:1]\n",
    "        v_BC_1_pred = BC_pred[:, 1:2]\n",
    "        u_BC_2_pred = BC_pred[:, 2:3]\n",
    "        v_BC_2_pred = BC_pred[:, 3:4]\n",
    "\n",
    "        loss_u_BC_1 = loss_fn(u_BC_1, u_BC_1_pred)\n",
    "        loss_v_BC_1 = loss_fn(v_BC_1, v_BC_1_pred)\n",
    "        loss_u_BC_2 = loss_fn(u_BC_2, u_BC_2_pred)\n",
    "        loss_v_BC_2 = loss_fn(v_BC_2, v_BC_2_pred)\n",
    " \n",
    "        # Initial values evaluation\n",
    "        IV_pred = model([x_IV, t_IV], training=True) \n",
    "        u_IV_1_pred = IV_pred[:, 0:1] \n",
    "        v_IV_1_pred = IV_pred[:, 1:2] \n",
    "        u_IV_2_pred = IV_pred[:, 2:3] \n",
    "        v_IV_2_pred = IV_pred[:, 3:4] \n",
    "        \n",
    "        loss_u_IV_1 = 1000000*loss_fn(u_IV_1, u_IV_1_pred)\n",
    "        loss_v_IV_1 = 1000000*loss_fn(v_IV_1, v_IV_1_pred)\n",
    "        loss_u_IV_2 = 1000000*loss_fn(u_IV_2, u_IV_2_pred)\n",
    "        loss_v_IV_2 = 1000000*loss_fn(v_IV_2, v_IV_2_pred)\n",
    "        \n",
    "        # Collocated points evaluation - Differential Equation \n",
    "        with tf.GradientTape(persistent=True) as tape_diff:\n",
    "            tape_diff.watch(x)\n",
    "            tape_diff.watch(t)\n",
    "            pred = model([x, t], training=True)\n",
    "            u_1_pred = pred[:, 0:1]\n",
    "            v_1_pred = pred[:, 1:2]\n",
    "            u_2_pred = pred[:, 2:3]\n",
    "            v_2_pred = pred[:, 3:4]\n",
    "            \n",
    "            \n",
    "            u_1_x  = tape_diff.gradient(u_1_pred, x)\n",
    "            u_2_x  = tape_diff.gradient(u_2_pred, x)\n",
    "            v_1_x = tape_diff.gradient(v_1_pred, x)\n",
    "            v_2_x = tape_diff.gradient(v_2_pred, x)\n",
    "            u_1_t = tape_diff.gradient(u_1_pred, t)\n",
    "            v_1_t = tape_diff.gradient(v_1_pred, t)\n",
    "            u_2_t = tape_diff.gradient(u_2_pred, t)\n",
    "            v_2_t = tape_diff.gradient(v_2_pred, t)\n",
    "\n",
    "        del tape_diff\n",
    "        \n",
    "\n",
    "        f_u_1 = u_1_t + u_2_x - v_1_pred\n",
    "        f_v_1 = v_1_t + v_2_x + u_1_pred\n",
    "        f_u_2 = -u_2_t - u_1_x - v_2_pred\n",
    "        f_v_2 = -v_2_t - v_1_x + u_2_pred\n",
    "    \n",
    "        \n",
    "        loss_f_u_1 = loss_fn(0, f_u_1)\n",
    "        loss_f_v_1 = loss_fn(0, f_v_1)\n",
    "        loss_f_u_2 = loss_fn(0, f_u_2)\n",
    "        loss_f_v_2 = loss_fn(0, f_v_2)\n",
    "        \n",
    "        h_1_pred = u_1_pred**2 + v_1_pred**2\n",
    "        h_2_pred = u_2_pred**2 + v_2_pred**2\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # General loss function\n",
    "        loss = loss_f_u_1 + loss_f_v_1 + loss_f_u_2 + loss_f_v_2 + loss_u_BC_1 + loss_u_BC_2 + loss_v_BC_1 + loss_v_BC_2 + loss_u_IV_1 + loss_v_IV_1 + loss_u_IV_2 + loss_v_IV_2\n",
    "        \n",
    "    # Regular backpropagation\n",
    "    grads = tape.gradient(loss, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    \n",
    "    \n",
    "    return loss,loss_f_u_1 ,loss_f_v_1, loss_f_u_2, loss_f_v_2, loss_u_BC_1, loss_u_BC_2, loss_v_BC_1, loss_v_BC_2, loss_u_IV_1, loss_v_IV_1, loss_u_IV_2, loss_v_IV_2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Callback_EarlyStopping(LossList, min_delta=0.1, patience=20):\n",
    "    #No early stopping for 2*patience epochs \n",
    "    if len(LossList)//patience < 2 :\n",
    "        return False\n",
    "    #Mean loss for last patience epochs and second-last patience epochs\n",
    "    mean_previous = np.mean(LossList[::-1][patience:2*patience]) #second-last\n",
    "    mean_recent = np.mean(LossList[::-1][:patience]) #last\n",
    "    #you can use relative or absolute change\n",
    "    delta_abs = np.abs(mean_recent - mean_previous) #abs change\n",
    "    delta_abs = np.abs(delta_abs / mean_previous)  # relative change\n",
    "    if delta_abs < min_delta :\n",
    "        print(\"*CB_ES* Loss didn't change much from last %d epochs\"%(patience))\n",
    "        print(\"*CB_ES* Percent change in loss value:\", delta_abs*1e2)\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 500\n",
    "start_time_outer = time.time()\n",
    "loss_seq = []\n",
    "loss_seq = []\n",
    "for epoch in range(epochs):\n",
    "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "    start_time_inner = time.time()\n",
    "\n",
    "    # Iterate over the batches of the dataset.\n",
    "    for step, (x_batch_train, t_batch_train) in enumerate(train_dataset):\n",
    "        loss_train, loss_f_u_1, loss_f_v_1, loss_f_u_2, loss_f_v_2, loss_u_BC_1, loss_u_BC_2, loss_v_BC_1, loss_v_BC_2, loss_u_IV_1, loss_v_IV_1, loss_u_IV_2, loss_v_IV_2 = train_step(x_batch_train, t_batch_train)\n",
    "\n",
    "        # Log every 200 batches.\n",
    "        if step % 200 == 0:\n",
    "            print(f\"Training loss (for one batch) at step {step}: {format(float(loss_train), 'e')}\")\n",
    "            print(f\"f_u_1 loss (for one batch) at step {step}: {format(float(loss_f_u_1), 'e')}\")\n",
    "            print(f\"f_u_2 loss (for one batch) at step {step}: {format(float(loss_f_u_2), 'e')}\")\n",
    "            print(f\"f_v_1 loss (for one batch) at step {step}: {format(float(loss_f_v_1), 'e')}\")\n",
    "            print(f\"f_v_2 loss (for one batch) at step {step}: {format(float(loss_f_v_2), 'e')}\")\n",
    "            print(f\"u_BC_1 loss (for one batch) at step {step}: {format(float(loss_u_BC_1), 'e')}\")\n",
    "            print(f\"u_BC_2 loss (for one batch) at step {step}: {format(float(loss_u_BC_2), 'e')}\")\n",
    "            print(f\"v_BC_1 loss (for one batch) at step {step}: {format(float(loss_v_BC_1), 'e')}\")\n",
    "            print(f\"v_BC_2 loss (for one batch) at step {step}: {format(float(loss_v_BC_2), 'e')}\")\n",
    "            print(f\"u_IV_1 loss (for one batch) at step {step}: {format(float(loss_u_IV_1), 'e')}\")\n",
    "            print(f\"u_IV_2 loss (for one batch) at step {step}: {format(float(loss_u_IV_2), 'e')}\")\n",
    "            print(f\"v_IV_1 loss (for one batch) at step {step}: {format(float(loss_v_IV_1), 'e')}\")\n",
    "            print(f\"v_IV_2 loss (for one batch) at step {step}: {format(float(loss_v_IV_2), 'e')}\")\n",
    "            print(f\"Seen so far: {(step + 1) * batch_size} samples\")\n",
    "    \n",
    "    # Stop criteria\n",
    "    loss_seq.append(loss_train)  \n",
    "    stopEarly = Callback_EarlyStopping(loss_seq, min_delta=1e-02, patience=1000)\n",
    "    if stopEarly or loss_train < 1e-04:\n",
    "        print(\"Callback_EarlyStopping signal received at epoch= %d/%d\"%(epoch,epochs))\n",
    "        print(\"Terminating training \")\n",
    "        break\n",
    "        \n",
    "    \n",
    "    print(f\"Time taken: {round(time.time() - start_time_inner, 2)}s\")\n",
    "print(100*'=')\n",
    "print(f\"Total time taken: {round(time.time() - start_time_outer, 2)}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_hat = model.predict([x_values, time_test*np.ones(N_test)])\n",
    "h_hat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_1_hat = h_hat[:, 0:1]\n",
    "v_1_hat = h_hat[:, 1:2]\n",
    "u_2_hat = h_hat[:, 2:3]\n",
    "v_2_hat = h_hat[:, 3:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_test = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values = np.linspace(0, 1, N_test)\n",
    "time_test = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun(x,t,n, fn, fmn, gn, gmn, an,bn, Cn1, Cn3, Dn1, Dn3, etn, ex1n, ex2n, psi1, psi2):\n",
    "    fn = (2*z*(cmath.exp(2*z*np.pi*n)-1)*(3 - np.pi**2*n**2) + 6*np.pi*n*(cmath.exp(2*z*np.pi*n)+1))/24*np.pi**3*n**3\n",
    "    fmn = (2*z*(cmath.exp(-2*z*np.pi*n)-1)*(np.pi**2*n**2-3) + 6*np.pi*n*(cmath.exp(-2*z*np.pi*n)+1))/24*np.pi**3*n**3\n",
    "    gn = ((cmath.exp(2*z*np.pi*n)-1)*z*np.pi*n - 2*(cmath.exp(2*z*np.pi*n)+1) + 4*cmath.exp(z*np.pi*n))/8*np.pi**2*n**2\n",
    "    gmn = (-(cmath.exp(-2*z*np.pi*n)+1)*z*np.pi*n - 2*(cmath.exp(-2*z*np.pi*n)+1) + 4*cmath.exp(z*np.pi*n))/8*np.pi**2*n**2\n",
    "    an = math.sqrt(math.sqrt(4*np.pi**2*n**2 + 1)-1)/math.sqrt(math.sqrt(4*np.pi**2*n**2 - 1)+1)\n",
    "    bn = math.sqrt(math.sqrt(4*np.pi**2*n**2 + 1)+1)/math.sqrt(math.sqrt(4*np.pi**2*n**2 - 1)-1)\n",
    "    Cn1 = (gn + fn*bn)/(an + bn)\n",
    "    Cn3 = (gn + fn*an)/(an + bn)\n",
    "    Dn1 = (gmn + fmn*bn)/(an+bn)\n",
    "    Dn3 = (gmn + fmn*an)/(an+bn)\n",
    "    etn = cmath.exp(z*math.sqrt(4*np.pi**2*n**2 + 1)*t)\n",
    "    ex1n = cmath.exp(2*z*np.pi*n*x)\n",
    "    ex2n = cmath.exp(-2*z*np.pi*n*x)\n",
    "    psi1 = (Cn1 + Cn3)*ex1n*etn + (Dn1 + Dn3)*ex2n*etn\n",
    "    psi2 = (-an*Cn1 + bn*Cn3)*ex1n*etn + (an*Dn1 - bn*Dn3)*ex2n*etn\n",
    "    return fn, fmn, gn, gmn, an,bn, Cn1, Cn3, Dn1, Dn3, etn, ex1n, ex2n, psi1, psi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "\n",
    "im = plt.figure(figsize=(15,7))\n",
    "plt.plot(x_values, psi_1(x_values,time_test), '-', label='Analytical solution')\n",
    "plt.plot(x_values, u_1_hat, '-o', label='Neural network prediction')\n",
    "plt.xlabel('$\\\\xi$')\n",
    "plt.ylabel('u_1($\\\\xi,\\\\tau$)')\n",
    "plt.ylim(-4, 4)\n",
    "plt.title('Exact solution')\n",
    "plt.title('Example - 1, $\\\\omega_{1} = \\ldots = \\omega_{4} = 1000000$,$\\\\omega_{5} = \\ldots = \\omega_{12} = 1$')\n",
    "plt.legend()\n",
    "plt.grid()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
